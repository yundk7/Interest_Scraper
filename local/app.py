# 1. import Flask
from flask import Flask, request, render_template, session, redirect, jsonify
import pandas as pd
from splinter import Browser
import requests
from bs4 import BeautifulSoup
import re
import operator as op
from urllib.request import urlopen
from bs4 import BeautifulSoup

# from IPython.display import Image
# from IPython.core.display import HTML
from time import sleep

# import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.xception import (
    Xception, preprocess_input, decode_predictions)
from PIL import Image
import requests
from io import BytesIO

from tensorflow import keras
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg19 import (
    VGG19, 
    preprocess_input, 
    decode_predictions
)
import plotly as py
import plotly.express as px

pd.set_option('display.max_colwidth', -1)
url = 'https://youtube.com'

# #using beautifulsoup only, splinter not working for for loop
# response = requests.get(url)
# soup = BeautifulSoup(response.text, 'html.parser')
# news = soup.find_all("h3",class_="title")
# #extracting href and combining with url to get news url and append in list
# news_urls = []
# for n in range(0,3):
#     news_urls.append(url + news[n].a['href'])
# news_df = pd.DataFrame()
# for u in news_urls:
#     response = requests.get(u)
#     soup = BeautifulSoup(response.text, 'html.parser')
#     title = str(soup.find_all("h1",class_="article_title")).splitlines()[1]
#     rel_date = soup.find_all("span",class_="release_date")
#     rel_date = str(rel_date).splitlines()[1][2:]
#     para = soup.find_all("p")
#     para = str(para[1]).replace("<p>","").replace("</p>","").replace("â€”","")
#     df = pd.DataFrame({"date":[rel_date],"title":[title],"content":[para]})
#     news_df = news_df.append(df)
# news_df.to_html("news.html")



# 2. Create an app, being sure to pass __name__
app = Flask(__name__)


# 3. Define what to do when a user hits the index route
@app.route("/")
def home():
    print("Server received request for 'Home' page...")
    return ("interest scraper")


# 4. Define what to do when a user hits the /about route

@app.route("/xception", methods=["GET", "POST"])
def xception():
    if request.method == "POST":
        instaid = request.form["instaid"]
    
        #scraping images with beautifulsoup
        url = "https://www.instagram.com/" + instaid
        browser = Browser('chrome')
        browser.visit(url)
        sleep(1)
        bs = BeautifulSoup(browser.html, 'html.parser')

        #finds all the images in website and puts url in df
        images = bs.find_all('img', {'src':re.compile('.jpg')})
        image_urls = []
        for image in images: 
            image_urls.append(str(image['src']))
        image_df = pd.DataFrame({"image":image_urls})
        
        import numpy as np
        import tensorflow as tf

        from tensorflow.keras.preprocessing import image
        from tensorflow.keras.applications.xception import (
            Xception, preprocess_input, decode_predictions)

        from PIL import Image
        import requests
        from io import BytesIO


        model = Xception(
            include_top=True,
            weights='imagenet')
        
        prds = []
        pcts = []
        for i in image_urls:
            url = i
            response = requests.get(url)
            img = Image.open(BytesIO(response.content))
            img = img.resize((299, 299), Image.NEAREST)
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)
            predictions = model.predict(x)
            prds.append(decode_predictions(predictions, top=1)[0][0][1])
            pcts.append(decode_predictions(predictions, top=1)[0][0][2])
#             cnns.append(decode_predictions(predictions, top=3)[0])
        image_df["predictions"] = prds
        image_df["%"] = pcts
        image_df.sort_values("%",ascending = False,inplace = True)
        image_df.reset_index(drop=True,inplace=True)
        
#         from IPython.display import Image
#         from IPython.core.display import HTML
        
#         def path_to_image_html(path):
#             return '<img src="'+ path + '" width="300" >'
        
#         print("Server received request for 'About' page...")
#         return (image_df.to_html(escape=False ,formatters=dict(image=path_to_image_html)))
    
        gkey = "AIzaSyA-Rjp6nOeJp6815Xt1Kkuxc5XKMiKl_yA"
        depart =  request.form["depart"]
        target_radius = 1000
        
        records = pd.DataFrame()
        target_list = image_df.drop_duplicates(subset="predictions", keep="first")["predictions"]
        targets = str(depart).split(",")
        for target in targets:
                # Build the endpoint URL
            target_url = (f'https://maps.googleapis.com/maps/api/geocode/json?address={target}&key={gkey}')
            geo_data = requests.get(target_url).json()


            target_adr = geo_data["results"][0]["formatted_address"]
            lat = geo_data["results"][0]["geometry"]["location"]["lat"]
            lng = geo_data["results"][0]["geometry"]["location"]["lng"]

            target_coordinates = str(lat) + "," + str(lng)
            
            
#             target_radius = 800
            target_type = ""

            # set up a parameters dictionary

            for target_search in target_list:
                params = {
                    "location": target_coordinates,
                    "keyword": target_search,
                    "radius": target_radius,
                    "type": target_type,
                    "key": gkey
                }

                # base url
                base_url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"

                # run a request using our params dictionary
                response = requests.get(base_url, params=params)
                places_data = response.json()

                n=0
                # while int(n) > len(places_data):
                while int(n) < len(places_data["results"]):
                    try:
                        price=places_data["results"][int(n)]["price_level"]
                    except KeyError:
                        price = "NA"
                    try:
                        link=places_data["results"][int(n)]["place_id"]
                    except KeyError:
                        link = "NA"
                    try:
                        score = places_data["results"][int(n)]["rating"]
                    except KeyError:
                        score = "NA"
                    try:
                        reviews = places_data["results"][int(n)]["user_ratings_total"]
                    except KeyError:
                        reviews = "NA"
                    try:
                        lat = places_data["results"][int(n)]["geometry"]["location"]["lat"]
                        lon = places_data["results"][int(n)]["geometry"]["location"]["lng"]
#                         dist = pd.read_html(f"http://boulter.com/gps/distance/?from={target_coordinates}&to={poi_coord}&units=k")
#                         distance = float(str(dist[1][1][1]).split(" ")[0])
                    except IndexError or TimeoutError:
                        distance = "NA"
#                         drive_url = f"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={target_coordinates}&destinations={poi_coord}&key=AIzaSyA-Rjp6nOeJp6815Xt1Kkuxc5XKMiKl_yA"
#                         drive_res = requests.get(drive_url).json()
#                         distance = drive_res["rows"][0]["elements"][0]["distance"]["value"]/1000
#                         duration= int(drive_res["rows"][0]["elements"][0]["duration"]["value"]/60)
#                     except KeyError:
#                         distance = "NA"
#                         drive_dur = "NA"
                    
#                     try:
#                         walk_url = f"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={target_coordinates}&destinations={poi_coord}&mode=walking&key=AIzaSyA-Rjp6nOeJp6815Xt1Kkuxc5XKMiKl_yA"
#                         walk_res = requests.get(walk_url).json()
#                         distance = walk_res["rows"][0]["elements"][0]["distance"]["value"]/1000
#                         walk_dur = int(walk_res["rows"][0]["elements"][0]["duration"]["value"]/60)
#                     except KeyError:
#                         walk_dur = "NA"
                    
#                     try:
#                         transit_url = f"https://maps.googleapis.com/maps/api/distancematrix/json?units=imperial&origins={target_coordinates}&destinations={poi_coord}&mode=transit&key=AIzaSyA-Rjp6nOeJp6815Xt1Kkuxc5XKMiKl_yA"
#                         transit_res = requests.get(transit_url).json()
#                         transit_dur = int(transit_res["rows"][0]["elements"][0]["duration"]["value"]/60)
#                     except KeyError:
#                         transit_dur = "NA"
                    
                    content = pd.DataFrame ({"depart":target_adr,"poi":target_search,
                                            "name":[places_data["results"][int(n)]["name"]],
                                            "score":score,
                                             "reviews":reviews,
                                             "price":price,
                                             "link":link,
                                            "address":[places_data["results"][int(n)]["vicinity"]],
                                           "lat":lat, "lon":lon})
#                                             "drive":duration,
#                                             "public":transit_dur,
#                                             "walk":walk_dur})
                    records = records.append(content)
                    n+=1
        records.reset_index(drop = True,inplace = True)
#         records["link"]=records["link"].apply(lambda x: '<a href="https://www.google.com/maps/place/?q=place_id:{0}">link</a>'.format(x))
#         return (records.to_html(escape=False))
        records["size"] = 10
        px.set_mapbox_access_token("pk.eyJ1IjoidGl2bWU3IiwiYSI6ImNrMWEwZDVtNDI4Zm4zYm1vY3o3Z25zejEifQ._yTPkj3nXTzor72zIevLCQ")
        fig = px.scatter_mapbox(records, lat="lat", lon="lon", color = "poi", size = "size", hover_name="name",zoom = 13)
        fig.update_layout(autosize=True,width=1500,height=750)
        #                           ,margin=go.layout.Margin(l=50,r=50,b=100,t=100,pad=4))

        return(py.offline.plot(fig,output_type="div"))

    return render_template("xception.html")

@app.route("/vgg19", methods=["GET", "POST"])
def vgg19():
    if request.method == "POST":
        instaid = request.form["instaid"]
    
        #scraping images with beautifulsoup
        url = "https://www.instagram.com/" + instaid
        browser = Browser('chrome')
        browser.visit(url)
        sleep(1)
        bs = BeautifulSoup(browser.html, 'html.parser')

        #finds all the images in website and puts url in df
        images = bs.find_all('img', {'src':re.compile('.jpg')})
        image_urls = []
        for image in images: 
            image_urls.append(str(image['src']))
        image_df = pd.DataFrame({"image":image_urls})
        
        import numpy as np
        import tensorflow as tf

        from tensorflow import keras
        from tensorflow.keras.preprocessing import image
        from tensorflow.keras.applications.vgg19 import (
            VGG19, 
            preprocess_input, 
            decode_predictions
        )

        from PIL import Image
        import requests
        from io import BytesIO


        model = VGG19(include_top=True, weights='imagenet')
        
        prds = []
        pcts = []
        for i in image_urls:
            url = i
            response = requests.get(url)
            img = Image.open(BytesIO(response.content))
            img = img.resize((224, 224), Image.NEAREST)
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)
            predictions = model.predict(x)
            prds.append(decode_predictions(predictions, top=1)[0][0][1])
            pcts.append(decode_predictions(predictions, top=1)[0][0][2])
#             cnns.append(decode_predictions(predictions, top=3)[0])
        image_df["prediction"] = prds
        image_df["%"] = pcts
        image_df.sort_values("%",ascending = False, inplace = True)
        
#         from IPython.display import Image
#         from IPython.core.display import HTML
        
        def path_to_image_html(path):
            return '<img src="'+ path + '" width="300" >'
        
        print("Server received request for 'About' page...")
        return (image_df.to_html(escape=False ,formatters=dict(image=path_to_image_html)))

    return render_template("vgg19.html")


if __name__ == "__main__":
    app.run(debug=True)
